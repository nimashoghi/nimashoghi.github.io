---
title: "SmaQ: Smart quantization for DNN training by exploiting value clustering"
collection: publications
permalink: /publication/2021-SmaQ-Smart-quantization-for-DNN-training-by-exploiting-value-clustering
excerpt: 'Introduces a smart quantization technique that reduces memory usage during neural network training by up to 6.7x while maintaining accuracy by exploiting the normal distribution properties of neural network values.'
date: 2021-01-01
venue: 'IEEE Computer Architecture Letters'
paperurl: 'https://doi.org/10.1109/LCA.2021.3108505'
authors: '<b>Nima Shoghi</b>, Andrei Bersatti, Moinuddin Qureshi, Hyesoon Kim'
citation: '<b>Nima Shoghi</b>, Andrei Bersatti, Moinuddin Qureshi, Hyesoon Kim, IEEE Computer Architecture Letters, 2021.'
full_citation: '<b>Nima Shoghi</b>, Andrei Bersatti, Moinuddin Qureshi, Hyesoon Kim, &quot;SmaQ: Smart quantization for DNN training by exploiting value clustering.&quot; IEEE Computer Architecture Letters, 2021.'
---
Introduces a smart quantization technique that reduces memory usage during neural network training by up to 6.7x while maintaining accuracy by exploiting the normal distribution properties of neural network values.

[Access paper here](https://doi.org/10.1109/LCA.2021.3108505){:target="_blank"}
